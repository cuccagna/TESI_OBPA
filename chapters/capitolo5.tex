%!TEX encoding = UTF-8 Unicode
%!TEX root = ./../main.tex
%!TEX TS-program = xelatex

\chapter{SVM} % Main chapter title
\label{cap:cinque}
La tesi è volta ad investigare lo scenario in cui si utilizzi un classificatore come base per costruire un Oracolo. Da quest esigenza nasce la necessità di utilizzare uno dei tradizionali metodi statistici di \textit{machine learning}. Nella fattispecie si deve risolvere un problema di \textbf{classificazione binaria} dato che il modello, una volta avvenuto l'addestramento, ha lo scopo di predire come accettante o rigettante , cioè appartenente o meno ad \ac{L} , il campione sotto esame.\\
Tra i moltissimi metodi statistici esistenti forse i più noti sono \textit{Random Forest, Recurrent Neural Network, Convolutional Neural Network} e \ac{SVM}. Tra questi un metodo che è stato molto utilizzato nell'apprendimento di linguaggi sono delle riadattazioni delle \textit{Reti Neurali Ricorrenti}, che si sono dimostrate adeguate allo scopo. In queste sede si è scelto di usare \ac{SVM} perchè rappresentano un classificatore molto potente, e sebbene esistano dei lavori inerenti il tema [RIFERIMENTO UNIVERSAL KERNEL] non sono stati contestualizzati nell'ambito dell'\textit{active learning} per \ac{IIR} oppure sono limitati all'apprendimento di alcune specifiche classi di linguaggi[RIFERIMENTI PLANAR LANGUAGES and LEARNABILITY E DE LA HIGUERA]

\section{Overview teorica}
Il problema che si tenta di risolvere con le \ac{SVM} , che è la radice da cui nascono anche altri modelli , appare sotto diversi nomi come il compromesso bias-varianza[INSERIRE RIFERIMENTO SE LO METTO IN TESI], controllo della capacità e dell'\textit{overfitting} dei dati, ma l'idea è la stessa: dato un problema di \textbf{apprendimento supervisionato}, cioè con un insieme di addestramento di cardinalità finita che contiene i dati etichettati con la rispettiva classe di appartenenza (il \textit{training set}) , le migliori \textit{performances} di generalizzazione\footnote{Cioè una migliore capacità di predire correttamente la classe di appartenenza di dati mai visti, cioè non contenuti nel \textit{training set}. Per questo motivo si parla di classificazione.} del classificatore saranno ottenute dal raggiungimento del giusto equilibrio tra l'\textit{accuracy} sul training set del classificatore e la capacità del modello. La capacità di un modello di classificazione statistico rappresenta la complessità, la potenza espressiva, la ricchezza piuttosto che la flessibilità della famiglia di funzioni $f(x,\alpha)$:
\begin{equation*}
f : X \to Y \quad \text{con } X=\{x_{1},x_{2},\dots,x_{l}\} 
\end{equation*}
$X$ contiene i campioni e rappresenta il \textit{training set} e $Y$ rappresenta i possibili valori delle etichette di ciascun campione e nel caso di \textbf{classificazione binaria} $Y=\{0,1\}$. Inoltre si parla di famiglia di funzioni perchè \ac{SVM} (come gli altri modelli) ha una serie di iperparametri ,rappresentati da $\alpha$ , che identificano i diversi classificatori\footnote{Classificatore e modello non sono termini interscambiabili. Il modello è rappresentato dalle funzioni $f(x,\alpha)$ con $\alpha$ generico ,un classificatore invece è una singola funzione che si ottiene dal modello per una specifica istanza degli \textit{iperparametri} $\alpha$.}.  Una capacità alta implica che le funzioni $f(x,\alpha)$ sono complesse cioè di alto grado ad esempio: il discorso è analogo a ciò che accade in una feedback-forward con tante unità nascoste. Per il principio del \textit{Rasoio di Occam} [INSERIRE RIFERIMENTO AL CAPITOLO 2 DOVE SI PARLA DEL RASOIO] è bene selezionare il classificatore con la capacità minima che contemporaneamente si adatta ai dati del \textit{training set} ma come si vedrà i due aspetti sono in contrapposizione e sarà necessario trovarne il giusto compromesso. Questo assunto più avanti sarà approfondito e giustificato. Per il momento basti dire che un classificatore con troppa capacità è come un botanista con una memoria fotografica che quando vede un albero mai visto --- cioè appartenente a quello che formalmente viene identificato come \textit{test set} --- conclude che non è un albero perchè ha un numero di foglie diverso dagli alberi visti finora: situazione detta di \textit{overfitting} in cui il classificatore si è adattato esclusivamente ai dati del \textit{training set} ma non generalizza (analogo della rete neurale con troppe unità nascoste rispetto alla cardinalità del \textit{training set}). D'altro canto una macchina con bassa capacità è come il fratello pigro del botanista  che classifica qualunque cosa sia verde come un albero. In nessuno dei due casi si avrà una buona generalizzazione.

\subsection{Rischio Atteso}
Supponiamo di avere $l$ osservazioni nel \textit{training set}. Ogni osservazione è una coppia: un vettore $x_{i} \in \mathbb{R}^{n} \text{ , } i=1,\cdots,l$ e l'etichetta corretta corretta associata $y_{i}$. Per semplicità $y_{i} \in \{-1,1\}$. Si assume che le coppie $(x_i,y_i)$ siano estratte da una distribuzione di probabilità cumulativa (CDF) che si indica con $P(x,y)$,  e con $p(x,y)$ la corrispondente densità di probabilità (PDF).  Supponiamo che il modello sta provando ad imparare il mapping $x_i \to y_i \text{ per } i=1,\dots,l$.  Il modello è definito da una serie di possibili mapping $f(x,\alpha) \text{ tali che } x \to f(x,\alpha) $. Il modello rappresenta la classe di funzioni $f(x,\alpha) $, si parla di classe perchè più funzioni possono essere imparate dal modello cambiando gli \textit{iperparametri} $\alpha$ e fissare $\alpha$ (che possono essere più di uno) per un modello si concretizza in un modello ``trained'', cioè in uno specifico classificatore. Ad esempio se il modello è una rete neurale gli \textit{iperparametri} $\alpha$ sono tipicamente i pesi e il bias. \\ Seguendo il riferimento \cite{Vapnik95}, si può definire il \textbf{rischio atteso} per un classificatore ($\alpha$ fissato) come:
\begin{equation}
\label{eqn:rischioatteso}
R(\alpha) = \int \frac{1}{2} \abs{y-f(x,\alpha)} \, dP(x,y) = \int \frac{1}{2} \abs{y-f(x,\alpha)} \, p(x,y) \, dx \, dy
\end{equation}
Il rischio atteso è anche detto \textbf{true mean error}  dato che è calcolato su tutti i possibili valori di $x$ e $y$ ($X \times Y$) cioè tiene conto di tutte le combinazioni sia sulle coppie nel \textit{training set} sia di tutte quelle mai osservate. Inoltre nell'equazione \eqref{eqn:rischioatteso}  $1/2 \abs{y-f(x,\alpha)}$ è una \textbf{funzione di loss} , ma se ne poteva scegliere un'altra in generale è $V(f(x,\alpha),y)$. La specifica funzione di loss scelta nell'equazione \eqref{eqn:rischioatteso} può assumere solo valori in $\mathbb{B}$. Tuttavia $R(\alpha)$ così definito non è utile in quanto quasi mai si conosce $P(x,y)$ o una sua stima ,allora si definisce il \textbf{rischio empirico} sempre per un classificatore:
\begin{equation*}
R_{emp}(\alpha) = \frac{1}{2l} \sum_{i=1}^{l}\abs{y-f(x_{i},\alpha)}
\end{equation*}
e rappresenta il tasso di errore medio sul \textit{training set}. $R(\alpha) - R_{emp}(\alpha)$ è l'errore di generalizzazione. Si dice che un modello generalizza se
\begin{equation*}
\lim_{ l \to \infty} R(\alpha) - R_{emp}(\alpha) = 0
\end{equation*}   
cioè al crescere degli elementi nel \textit{training set} gli elementi mal predetti nel \textit{test set} diminuiscono. Ma come già detto il rischio atteso non è quasi mai calcolabile per come è stato definito, allora con un'impostazione molto simile al \textit{PAC-learning} introdotto in [INSERIRE RIFERIMENTO TESI DOVE SI PARLA DEL PAC LEARNING] Vapnik ha dimostrato in \cite{Vapnik95}  che:

\begin{align}
  \intertext{Definiti} 
  \eta &\text{ : } 0 \leq \eta \leq 1 \notag \\ 
  h &\text{ : } h > 0 \\
  \upvarepsilon &= \sqrt{\biggl(\frac{h(\log (2l/h) + 1) - \log (\eta/4)}{l}\biggl)}\\ \intertext{allora si ha che:} 
  R&(\alpha) \leq R_{emp}(\alpha) +\upvarepsilon \quad \text{con probabilità }1-\eta \label{eqn:vapnik}
\end{align}

dove $h$ è detta \ac{VC} \textbf{dimension} e $\upvarepsilon$ è detto \textbf{learning rate} o \textbf{\ac{VC}} \textbf{\textit{confidence}}. La \ac{VC} \textit{dimension} è una misura della capacità e sarà approfondita nella sottosezione [INSERIRE RIFERIMENTO SOTTOSEZIONE VC DIMENSION].  $R_{emp}(\alpha) +\upvarepsilon$ è talvolta detto \textbf{\textit{risk bound}} dato che è un limite superiore del rischio atteso $R(\alpha)$.  Il risultato dell'equazione \eqref{eqn:vapnik} ci consente di trovare con probabilità $1-\eta$ il classificatore che minimizza $R_{emp}(\alpha) +\upvarepsilon$. Tuttavia calcolare questo minimo è molto difficile come è spiegato nella sottosezione [INSERIRE RIFERIMENTO SOTTOSEZIONE STRUCTURAL RISK MINIMIZATION]. Prima di parlarne si spiega cosa è la \ac{VC} \textit{dimension}.

\subsection{\textit{VC dimension}}
\label{sub:vcdim}
La \ac{VC} \textit{dimension} è una misura della capacità di un insieme di funzioni $\{f(x,\alpha)\}$ che possono essere apprese da un modello statistico, ed è definita come la cardinalità del più largo insieme di punti che un modello può \textbf{\textit{shatter}}. Per \textit{shatter} si intende che una delle funzioni è in grado di etichettare correttamente i punti o meglio di separare punti (che sarebbero dei campioni, delle osservature) con etichettature diverse. Ci si pone  nel caso della classificazione binaria e si fissa il codominio di $\{f(x,\alpha)\}$  in $\{-1,1\} \forall x,\alpha$.  Se un insieme $X$ di $l$ punti viene etichettato in tutti i $2^l$ possibili modi, e per ognuna delle $2^l$ combinazioni, può essere trovata una funzione (con $\alpha$ specifico cioè un classificatore) dell'insieme $\{f(x,\alpha)\}$ che separa (o assegna) correttamente queste etichette, si dice che l'insieme di punti $X$ è ``\textit{shattered}'' dall'insieme di funzioni. Quindi la \ac{VC} \textit{dimension} dell'insieme di funzioni $\{f(x,\alpha)\}$ è definita come il \textbf{massimo} numero di punti che possono essere ``\textit{shattered}'' da $\{f(x,\alpha)\}$. Si osservi che avere per esempio una \ac{VC} \textit{dimension}  pari a tre significa che esiste almeno un insieme fatto da tre punti che può essere ``\textit{shattered}'', ma non è detto (tipicamente non lo è) che tutti gli insiemi di tre punti lo siano\footnote{In questo caso si può solo concludere che non esiste nessun insieme di quattro punti che è ``\textit{shattered}''}. Si fornisce come esempio il calcolo delle \ac{VC} \textit{dimension} dell'insieme costituito da tutte le rette in $\mathbb{R}^2$ identificate dall'equazione $y = mx + q$ in cui i parametri $\alpha$ sono rappresentati da $m \text{ e } q$. I punti possono essere etichettati positivamente o negativamente, quindi per essere ``\textit{shattered}'' da una retta devono essere separati da quest'ultima in base all'etichettatura. Inoltre si rimarca che i punti possono essere collocati in qualunque maniera, ma una volta collocati le varie etichettature devono essere effettuate mantenendo fissa quella collocazione scelta. Con tre punti, si riesce a trovare una collocazione dei tre punti (qualsiasi tranne tre punti allineati) tale che esiste una retta (valori specifici di $m$ e $q$)  che separari i punti positivi da quelli negativi e si riesce a fare ciò per ciascuna delle $2^{3}=8$ etichettature diverse\footnote{Si noti che in queste 8 etichettature  la collocazione dei 3 punti deve restare costante ma si può scegliere un'altra funzione (retta) da un'etichettatura all'altra} come si può apprezzare in figura \ref{fig:sha}. 
\begin{figure}[htp]
	\centering
	\includegraphics[ width=0.7\textwidth]{RettaShatter}
	\caption[Tre punti shattered nel piano]{Tre punti in $\mathbb{R}^{2}$ , shattered da rette}
   \label{fig:sha}
\end{figure} 

Quattro punti non possono essere ``\textit{shattered}'' quindi la \ac{VC} \textit{dimension} è tre. Più in generale la \ac{VC} \textit{dimension} dell'insieme di iperpiani in $\mathbb{R}^{n}$ è $n+1$. Una dimostrazione di questo risultato si può trovare in \cite[p. 37]{Burges98}. 

\subsection{Structural Risk Minimization}
\label{sub:srm}
La \ac{VC} \textit{dimension} di un modello riveste un ruolo molto importante perchè come si evince dai risultati dell'equazione  \eqref{eqn:vapnik}  ,fissati $\eta \text{ e } l$, si ha che la \ac{VC} \textit{confidence} $\upvarepsilon$ aumenta all'aumentare della \ac{VC} \textit{dimension} $h$. Quindi per diminuire il rischio atteso $R(\alpha)$ sembra sufficiente diminuire $h$ ma in realtà non è così. 
Innanzitutto si precisa che un numero di parametri ($\alpha$) maggiore nel modello non implica  una \ac{VC} \textit{dimension} maggiore infatti esistono dei modelli con un solo parametro e \ac{VC} \textit{dimension} infinita. Inoltre possedere un valore di $h$ più piccolo non comporta necessariamente di avere un rischio atteso più piccolo e quindi un classificatore migliore infatti un valore di $h$ più piccolo significa che è stato ristretto l'insieme delle funzioni $\{f(x,\alpha)\}$ e può accadere che è stata eliminata proprio la funzione che minimizzava il rischio empirico. Quindi dinimuendo $h$ la \ac{VC} \textit{confidence} $\upvarepsilon$ diminuisce ma il rischio empirico può aumentare e quindi diminuire $h$ non implica che il rischio atteso $R(\alpha)$ diminuisca\footnote{Esistono modelli che hanno buoni riscontri pratici nonostante abbiano $h = \infty$}. Il motivo di questo comportamento è che la \ac{VC} \textit{confidence} dipende dalla classe di funzioni $\{f(x,\alpha)\}$ invece il rischio empirico dipende dalla scelta degli \textit{iperparametri} e quindi da una specifica funzione. \\ Al fine di minimizzare $R(\alpha)$ si deve trovare il giusto compromesso tra due quantità che manifestano un andamento opposto al variare di $h$: il rischio empirico (che diminuisce all' aumentare della complessità di  $\{f(x,\alpha)\}$ cioè della capacità del modello cioè all'aumentare di $h$) e la \ac{VC} \textit{confidence} (che diminuisce al diminuire di $h$). Vapnik in \cite{Vapnik82} ha proposto una procedura per affrontare il problema appena delineato detta \ac{SRM} che consiste nel:
\begin{enumerate}
\item Dividere la famiglia di funzioni $\{f(x,\alpha)\}$ in sottoinsiemi autoincludenti come in figura \ref{fig:ssrm} in modo che i sottoinsiemi abbiano una \ac{VC} \textit{dimension} (che è un valore intero) crescente. Ciò significa escludere alcune funzioni da $\{f(x,\alpha)\}$ in modo da trovare la classe di funzioni $h_1$ con \ac{VC} \textit{dimension}  più piccola e così via per $h_{2} \text{,}h_{3}\dots$

\item Per ogni classe di funzioni trovate al punto 1. ($h_{1} \text{,}h_{2}\dots$) trovare quella con il rischio empirico minimo: cioè equivale a una selezione dei parametri. Ad esempio per la classe $h_1$ significa trovare la funzione contenuta in $\{h_{1}(x,\alpha)\}$ che minimizza il rischio empirico (ciè equivale a trovare i parametri $\alpha$ che fanno ciò).

\item Selezionare il classificatore tra quelli trovati al punto 2. che minimizza $R(\alpha)$
\end{enumerate}

\ac{SRM} garantisce un compromesso tra la qualità dell'approssimazione dei dati nel \textit{training set} e la complessità della funzione approssimante come illustrato in figura \ref{fig:srm}

\begin{figure}[htp]
	\centering
	\includegraphics[ width=0.7\textwidth]{SRM}
	\caption[Principio della SRM]{Il \textit{risk bound} è la somma del rischio empirico e la \ac{VC} \textit{confidence}. Il più piccolo \textit{risk bound} è raggiunto su qualche elemento della struttura. }
   \label{fig:srm}
\end{figure} 

\ac{SRM} non è quasi mai applicabile perchè è molto complicato riuscire a calcolare la \ac{VC} \textit{dimension}  delle ``sottofunzioni'' e anche se ciò fosse possibile la difficoltà computazionale per calcolare il rischio empirico , a causa della spesso grande dimensionalità dello spazio degli \textit{iperparametri} del modello, è insostenibile. Si vedrà [INSERIRE RIFERIMENTO DOVE SI PARLA DI COME SVM USI IMPLICITAMENTE SRM,ALALOGO, PER MINIMIZZARE IL RISCHIO ATTESO] che il principio di funzionamento delle \ac{SVM} ricalca concettualmente  da vicino quello della \ac{SRM}.

 \begin{figure}[htp]
	\centering
	\includegraphics[ width=0.7\textwidth]{SottoinsiemiSRM}
	\caption[Sottoinsiemi SRM]{Famiglia di funzioni del modello suddivise in base alla \ac{VC} \textit{dimension} }
   \label{fig:ssrm}
\end{figure}

\section{SVM}
Support Vector Machines (\ac{SVM}) sono un metodo d'apprendimento supervisionato, introdotte per la prima volta da Vapnik e Chervonenkis nel 1963. Nel corso degli anni sono state introdotte varie versioni di \ac{SVM} tra cui ne sono rimarcabili quattro:
\begin{enumerate}
\item Quella originale: Il \textit{Maximal Margin Classifier}.
\item La versione con il \textbf{\textit{kernel}}.
\item La versione detta \textbf{\textit{soft-margin}}.
\item La versione \textit{soft-margin} con \textit{kernel} che combina i tre punti precedenti.
\end{enumerate}
La letteratura sull'argomento è sterminata e \cite{Osuna97} \cite{Burges98} \cite{Ng}  sono solo alcuni dei riferimenti esistenti. Da essi si è tratto gran parte del materiale presentato in questa sezione.
 Le \ac{SVM} consentono di affrontare essenzialmente tre tipi di problemi:
\begin{itemize}
\item Classificazione binaria
\item Classificazione con più classi
\item Regressione lineare
\end{itemize}   

La classificazione binaria è naturale con \ac{SVM} ed è adatta a quei problemi in cui i dati possono appartenere a due classi distinte (etichettate di solito con 1 o -1).\\
Nella classificazione con più classi i dati possono appartenere a più classi.
Nella regressione lineare lo scopo è determinare una funzione lineare che meglio approssimi  un insieme di dati.\\
Il problema affrontato in questa tesi attiene ai problemi di classificazione binaria.

\subsection{Classificazione binaria}
\label{sub:clb}
Un problema di classificazione per molti versi è simile a un problema di inferenza induttiva [INSERIRE RIFERIMENTO TESI]. Da una serie di osservazioni cioè di elementi del \textit{training set} X si deve estrarre il migliore classificatore dallo spazio delle ipotesi $f(x,\alpha)$ secondo qualche criterio di preferenza. Gli elementi del training set in un problema di classificazione binaria supervisionato sono del tipo $(x_i , y_i)$ con $i=1,\dots,l$ cioè $\abs{X}=l$ , $x_i \in \mathbb{R}^{n}$ cioè ogni singolo elemento del \textit{training set} ha dimensionalità $n$, e $y_i \in Y=\{-1,1\}$ (o $\mathbb{B}$ in alcuni casi). La dicotomia delle etichette dell'insieme $Y$ rende la classificazione come \textit{binaria}.
 Nel caso delle \ac{SVM} le funzioni sono del tipo:
\begin{equation*}
f : X \subseteq \mathbb{R}^{n} \to \mathbb{R}
\end{equation*}
Da un punto di vista geometrico la classificazione binaria nelle \ac{SVM} consiste nel trovare l' iperpiano ottimo ,secondo qualche criterio, nello spazio $\mathbb{R}^n$  che separi i dati del \textit{training set} etichettati positivamente da quelli etichettati negativamente. Se almeno un iperpiano siffatto esiste i dati del \textit{training set} sono detti essere \textbf{linearmente separabili}. Un esempio in figura \ref{fig:lsd}\\
Da un punto di vista alanitico è necessario una funzione lineare che classifichi correttamente il \textit{training set}. Una funzione lineare,cioè un iperpiano,  è nella forma:
\begin{equation*}
f(x) = w \bullet x +b = \Biggl(\sum_{i=1}^{n}w_{i}x_{i}\Biggl) + b
\end{equation*}
Quindi la dipendenza dai parametri $\alpha$ si riconduce alla dipendenza da $w$ --- che è un vettore nello spazio $\mathbb{R}^n$ che rappresenta la normale all'iperpiano --- e $b$ uno scalare che consente all'iperpiano di muoversi parallelamente a se stesso.\\  
Una volta scelto il classificatore dallo spazio delle ipotesi, l'ingresso $x=(x_1,x_2,\dots,x_n) $ del \textit{test set} (cioè di un insieme di elementi non sottoposti in fase di addestramento al modello) sarà classificato in base all'esito della funzione $sign(f(x))$ cioè associato alla classe positiva se $f(x) \geq 0$, altrimenti sarà associato alla classe negativa.
 
 \begin{figure}[htp]
	\centering
	\includegraphics[ width=0.5\textwidth]{linearlyseparabledata}
	\caption[Lineare separabilità]{Dati in $\mathbb{R}^{2}$ linearmente separabili}
   \label{fig:lsd}
\end{figure}

\subsection{Classificatore  a margine massimo}
\label{sub:cmm}
Il \textit{Maximal Margin Classifier} rappresenta la versione iniziale di \ac{SVM} e spesso in letteratura questa tecnica è nota anche come \textbf{hard margin}. Si applica quando i dati sono \textbf{linearmente separabili}. Come accennato nella sottosezione \ref{sub:clb} è necessario trovare il ``migliore'' iperpiano che separa linearmente i dati nel \textit{training set}. Ai nostri scopi per lineare separabilità si intende che si può trovare una coppia $(w,b)$ tale che i seguenti vincoli sono rispettati:
\begin{align*}
&w \bullet x_i + b \geq \:\:\:1 \quad \quad \qquad\text{ per } y_i = +1 \:\:\:\forall i\\
&w \bullet x_i + b \leq -1 \quad \quad \qquad\text{ per } y_i = -1 \:\:\:\forall i
\end{align*}
I due vincoli possono essere convenientemente combinati insieme in modo da ottenere un unico vincolo più compatto:
\begin{equation}
\label{eq:vin}
y_i(w \bullet x_i + b) - 1 \geq 0 \qquad \qquad \forall i
\end{equation}
Il vincolo \eqref{eq:vin}  deriva dalla constatazione che la \textit{\textbf{decision function}} $sign(w \bullet x + b)$, cioè l'iperpiano separatore ,rispettante il vincolo \eqref{eq:vin}, che viene selezionato come migliore , è tale che se $w$ e $b$ sono scalati\footnote{Per scalati si intende moltiplicati} per la stessa quantità scalare $\alpha \in \mathbb{R}^{+}$  il vincolo \eqref{eq:vin} è ancora rispettato. Dunque per eliminare questa ridondanza , e per rendere ogni \textit{decision function} corrispondente ad un'unica coppia $(w,b)$, viene imposto il seguente vincolo:
\begin{equation}
\label{eq:vin2}
\min_{i=1,\dots,l}^{}\abs{w \bullet x_{i} + b} = 1
\end{equation}
che è un modo equivalente di scrivere il vincolo \eqref{eq:vin}.  Nel gergo tecnico delle \ac{SVM} si suole dire che si è scelto un parametro $\alpha$ tale che il \textit{margine funzionale dell'iperpiano}\footnote{Informalmente il \textit{margine funzionale dell'iperpiano} è la distanza minima tra tutti i punti del \textit{training set} e l'iperpiano separatore} è pari a 1 cioè la valutazione della \textit{\textbf{decision function}} nei punti del \textit{training set}  più vicini all'iperpiano separatore è tale che\footnote{Si può dimostrare che l'esistenza di tali punti è assicurata}
\begin{align}
&w \bullet x_i + b = \:\:\:1 \qquad \qquad\text{ per } y_i = +1 \label{eqn:h1}\\
&w \bullet x_i + b = -1 \qquad \qquad\text{ per } y_i = -1  \label{eqn:h2}
\end{align}
L'nsieme di iperpiani che soddisfano \eqref{eq:vin} sono detti \textbf{Iperpiani Canonici}\footnote{In molte trattazioni si definisce il margine geometrico come il margine funzionale rispetto all'iperpiano normalizzato rispetto a $\norma{w}$ e poi si dimostra che margine funzionale e margine geometrico coincidono imponendo il vincolo  \eqref{eq:vin}}.  Per un'\textit{iperpiano canonico} in cui il margine geometrico e il \textit{margine funzionale dell'iperpiano} coincidono si può dare una definizione intuitiva di \textbf{margine}.
\begin{definizione}
\label{def:mar}
Si indichi con $d_+$ la più breve distanza dell'iperpiano separatore dal più vicino esempio positivo del \textit{training set} e con $d_{-}$ la più breve distanza dall'esempio negativo più vicino del \textit{training set}. Allora:
\begin{equation*}
margine = d_{+} + d_{-}
\end{equation*}
\end{definizione}

Si può dimostrare che la distanza di un punto $x$ da un iperpiano $w \bullet x +b$ è pari a:
\begin{equation*}
d = \frac{\abs{w \bullet x + b}}{\norma{w}}
\end{equation*}
In accordo alla normalizzazione definita in \ref{eq:vin2} la distanza tra l'\textit{iperpiano canonico} e il più vicino degli elementi del \textit{training set} è $\frac{1}{\norma{w}}$. Quindi il margine di un \textit{iperpiano separatore canonico} secondo la definizione \ref{def:mar} è $\frac{2}{\norma{w}}$. L'immagine [INSERIRE RIFERIMENTO IMMAGINE] può essere chiarificatrice.\\

\begin{figure}[htp]
	\centering
	\includegraphics[ width=0.7\textwidth]{Margine}
	\caption[Esempio iperpiano separatore]{Iperpiano separatore per dati linearmente separabili. $H_{1}$ e $H{2}$ sono paralleli dato che hanno la stessa normale $w$ come si apprezza nelle equazioni \ref{eqn:h1} e \ref{eqn:h2}}
   \label{fig:lsd}
\end{figure}

Lo scopo di una \ac{SVM} è:
\begin{enumerate}
\item \label{ite:obj}classificare correttamente il \textit{training set}
\item e selezionare tra quelli che rispettano il punto \ref{ite:obj} quello che generalizza meglio.
\end{enumerate}

Detto succintamente il goal di una \ac{SVM} è trovare l'iperpiano canonico ottimo che separa il \textit{training set} , dove per ottimo si intende quello che massimizza il margine.\\

\subsubsection{Legame con SRM}
\label{subsub:lsrm}
In questa sottosezione si vedrà come la tecnica \textbf{hard margin} di \ac{SVM} è in stretta relazione con \ac{SRM} introdotto in [INSERIRE PARTE TESI DOVE  PARLO di SRM]. Si ha infatti che essendo i dati linearmente separabili la rispondenza con il training set è totale quindi $R_{emp}=0$. Quindi il rischio atteso dipende unicamente dalla \ac{VC} \textit{confidence} che a sua volta dipende dalla \ac{VC} \textit{dimension} $h$. Quindi il classificatore migliore nel caso di \ac{SVM} sarà quello con \ac{VC} \textit{dimension} $h$ minima.  La \ac{SRM} in questo caso diventa una ricerca del classificatore con \ac{VC} \textit{dimension} minima. Si consideri inoltre il seguente teorema:
\begin{teorema}
\label{teo:suphdim}
Sia X un insieme di punti x di uno spazio n-dimensionale  che contiene tutti gli esempi di apprendimento. Sia R il diametro della più piccola ``palla'' (da pensare n-dimesionale) centrata nell'origine che contiene tutti i punti di X. Allora la \ac{VC} dimension ,h, dell'insieme di iperpiani $w \bullet x + b = 0$ aventi margine $\gamma$ è limitata superiormente da: 
\begin{equation*}
h \leq min \biggl( \ceil*{\frac{R^{2}}{\gamma^{2}}} \:\:,\:\: n \biggl) + 1
\end{equation*}
\end{teorema}
Siccome il margine è $\gamma = 2/\norma{w}$ la \ac{VC} \textit{dimension} h più piccola (che abbiamo detto minimizza la \ac{VC} \textit{confidence} e avendo il $R_{emp}$ nullo minimizza anche il rischio atteso) si ottiene per la $\norma{w}$ minima cioè per il margine (pari a $2/\norma{w}$) massimo.  Da quanto detto segue che le \ac{SVM} ricercano l'iperpiano col margine massimo per ottenere il classificatore migliore che da cioè più garanzie di generalizzazione secondo il risultato dell'equazione \eqref{eqn:vapnik} e della \ac{SRM}.
Il discorso è analogo ,ma leggermente differente, nel caso in cui i dati non sono linearmente separabili come nel \textbf{soft-margin}[INSERIRE RIFERIMENTO]: in questo caso l'errore empirico non è nullo e le \ac{SVM} cercano per il migliore compromesso tra gli errori nel \textit{training set} e la massimizzazione del margine nell'ottica di minimizzare il \textit{risk boud} $R(\alpha)$.

\subsubsection{Formulazione matematica}
Dalla sottosezione \ref{subsub:lsrm} si evince che \ac{SVM} deve risolvere un problema di programmazione lineare infatti il goal è massimizzare il margine  soggetto a dei vincoli lineari cioè un problema di massimizzazione con vincoli. Ricapitolando dato il \textit{training set} $X$ è necessario trovare tra tutti gli iperpiani che separano i dati (che sono linearmente separabili) quello che massimizza il margine, che è pari a $\frac{2}{\norma{w}}$.  Matematicamente si ha:
\begin{alignat*}{2}
&massimizzare \quad&&\frac{2}{\norma{w}} \\
&soggetto \:\:a &&y_i(w \bullet x_i +b) - 1 \geq 0 \qquad \forall i
\end{alignat*} 

Osservando che $\norma{w}^{2} = w \bullet w$ si può dare una formulazione  alternativa ma equivalente che cambia leggermente la natura del problema rendendolo quadratico ma è più conveniente come si vedrà più avanti:
\begin{alignat}{2}
\label{eq:svmpro1}
&minimizzare \quad&&\frac{1}{2}(w \bullet w) \\
\label{eq:svmpro2}
&soggetto \:\:a &&y_i(w \bullet x_i +b) - 1 \geq 0 \qquad \forall i
\end{alignat}
 Per risolvere un problema di \textbf{minimizzazione vincolato} si usano i \textbf{moltiplicatori di Lagrange}. Ad esempio si ha da $\underset{x,y}{minimizzare} \:\: f(x,y)$ col vincolo $g(x,y) = 0$ . Si ha inoltre che $g(x,y) = y+x-1$ allora si ha che $y = 1 - x$ che costituisce il cosiddetto \textbf{feasible set} cioè il luogo dei punti dove la soluzione può venire a trovarsi. Lagrange ha dimostrato che la soluzione si ha laddove il gradiente\footnote{Il gradiente è una quantità vettoriale, cioè un vettore espresso tramite le sue componenti. Per i nostri scopi le singole componenti sono formate facendo le derivate parziali della funzione rispetto a ogni singola variabile}  di $f(x,y)$  e quello di $g(x,y)$ puntano nella stessa direzione cioè 
 \begin{equation}
 \label{eq:lag}
 \nabla f(x,y) = \lambda \,\nabla g(x,y)
 \end{equation}
  La quantità scalare $\lambda$ si chiama \textbf{moltiplicatore di Lagrange} e si rende necessaria per il fatto che $\nabla f(x,y) \text{ e } \nabla g(x,y)$ devono avere la stessa direzione ma non necessariamente lo stesso modulo, $\lambda$ è quindi un fattore moltiplicativo che ne tiene conto in modo da eguagliare anche i moduli. Il metodo dei moltiplicatori di Lagrange è valido solo per vincoli di uguaglianza ($g(x,y) = 0$) e in questo caso nelle condizioni per la risoluzione non ci sarà alcun vincolo su $\lambda$. Si definisce la funzione di Lagrange:
 \begin{equation*}
 L(x,y,\lambda) = f(x,y) - \lambda g(x,y) 
 \end{equation*}
 allora 
 \begin{equation*}
 \nabla L(x,y,\lambda) = \nabla f(x,y) - \lambda \, \nabla g(x,y)
 \end{equation*}
e dall'equazione \eqref{eq:lag} si deduce che $\nabla L(x,y,\lambda) = 0$ cioè:
\[
\begin{cases}
\frac{\partial L}{\partial x} = 0 \\
\frac{\partial L}{\partial y} = 0 \\
\frac{\partial L}{\partial \lambda} = 0
\end{cases}
\]
Per trattare casi in cui sono presenti anche dei vincoli di iniguaglianza come nel caso delle \ac{SVM} (vedasi vincoli \eqref{eq:vin}) si utilizzano le \textbf{Karush-Kuhn-Tucker (KKT)} condizioni che consistono nell'imporre --- oltre alle condizioni del metodo dei moltiplicatori di Lagrange per i vincoli di uguaglianza che permangono --- che il moltiplicatore relativo ad ogni vincolo di ineguaglianza venga posto maggiore uguale a 0 , e le condizioni di complementarità che consistono nell'imporre che il prodotto tra un moltiplicatore e il corrispondente vincolo sia uguale a zero. Il metodo dei moltiplicatori di Lagrange  è un caso speciale (sono presenti solo vincoli di uguaglianza) delle KKT condizioni che invece hanno validità generale.
Ad esempio se avessimo due vincoli $g_{1}(x,y) \geq 0 \text{ e } g_{2}(x,y) \geq 0$ si avrebbe che $L(x,y,\lambda_{1},\lambda_{2}) = f(x,y) - \lambda_{1}\,g_{1}(x,y) - \lambda_{2}\,g_{2}(x,y)$ da cui si calcola il gradiente come fatto nell'esempio precedente ottenendo:
\[
\begin{dcases}
\frac{\partial L}{\partial x} = 0 \\
\frac{\partial L}{\partial y} = 0 \\
\frac{\partial L}{\partial \lambda_{1}} = 0 \\
\frac{\partial L}{\partial \lambda_{2}} = 0 \\
\lambda_{1} \geq 0 \\
\lambda_{2} \geq 0
\end{dcases}
\]
I vincoli di complementarità $\lambda_{1}g_{1}(x,y) = 0$ e $\lambda_{2}g_{2}(x,y) = 0$ non sono presenti perchè già posseduti (per via delle condizioni KKT e del segno delle funzioni $g_{1}$ e $g_{2}$ ) e quindi sarebbero ridondanti.\\
Adesso, alla luce di quanto è stato esposto, si risolverà il problema matematico posto da \ac{SVM} e delineato nelle equazioni \eqref{eq:svmpro1} e \eqref{eq:svmpro2}. Siccome si hanno $l$ vincoli (tanti quanti gli elementi nel \textit{training set}) si deve introdurre un moltiplicatore $\alpha_{i} \text{ con } i=1,\dots,l$ per ognuno degli $l$ vincoli d'ineguaglianza ottenendo il seguente lagrangiano:
\begin{equation}
\label{eq:pri}
L_{p} = \frac{1}{2}\norma{w}^{2} - \sum_{i=1}^{l}\alpha_{i}y_{i}(w \bullet x_{i} + b) + \sum_{i=1}^{l}\alpha_{i}
\end{equation}
dove l'ultima sommatoria scaturisce dal -1 dei vincoli in \eqref{eq:svmpro2}.
Adesso si deve imporre pari a zero $\nabla L_{p}$ ,e siccome è un vettore imporre pari a zero le singole derivate parziali , e imporre le KKT condizioni ottenendo:

\[
\begin{dcases}
\frac{\partial L_{p}}{\partial w} = 0 \\
\frac{\partial L_{p}}{\partial b} = 0 \\
\frac{\partial L_{p}}{\partial \alpha_{i}} = 0 \quad \forall i \\
 \alpha_{i} \geq 0 \quad \forall i \\
\alpha_{i}[y_{i}(w \bullet x_i + b) - 1] = 0 \quad \forall i \\
\end{dcases}
\]
\\\\
che svolgendo le derivate parziali diventa:

\begin{subequations}
\begin{numcases}{}
w - \sum_{i=1}^{l}\alpha_{i}y_{i}x_{i} =0 \label{wvar}\\
\sum_{i=1}^{l}\alpha_{i}y_{i} = 0 \label{bvar}\\
y_{i}(w \bullet x_i + b) - 1 = 0 \quad \forall i \notag \\
 \alpha_{i} \geq 0 \quad \forall i \notag \\
\alpha_{i}[y_{i}(w \bullet x_i + b) - 1] = 0 \quad \forall i \label{vecsup}
\end{numcases}
\end{subequations}

Il problema appena impostato è un problema di programmazione quadratico dato che la funzione obiettivo ($\frac{1}{2}\norma{w}^2$) è quadratica, ma è anche un \textbf{problema convesso di programmazione quadratica} perchè la funzione obiettivo è anche convessa essendo un paraboloide, ed i punti che soddisfano i vincoli cioè il \textit{feasible set} è pure un insieme convesso\footnote{Qualsiasi vincolo lineare definisce un insieme convesso, e un insieme di $N$ vincoli lineari simultanei definisce l'intersezione di $N$ insiemi convessi, che è ancora un insieme convesso.} Questo sta a significare ch e otteremo lo stesso risultato affrontando il problema \textbf{duale} (quello descritto finora è il problema \textbf{primario} ): anzichè minimizzare rispetto alle variabili $w \text{ e } b$ soggetto a vincoli che coinvolgono i moltiplicatori lagrangiani $\alpha$, si massimizza rispetto alle variabili $\alpha$ (variabili duali) soggetto alle relazioni ottenute prima per $w \text{ e } b$ cioè le equazioni \eqref{wvar} e \eqref{bvar} oltre che ad $\alpha_{i} \geq 0$. Alla formulazione duale si possono aggiungere le KKT condizioni della forma primaria che ci forniscono ulteriori informazioni per comprendere la struttura della soluzione. La condizione  \eqref{vecsup} :
\[
\alpha_{i}[y_{i}(w \bullet x_i + b) - 1] \quad \forall i
\]
ci dice che solo i punti del \textit{training set} $x_i$ per i quali $y_{i}(w \bullet x_i + b) = 1$ ,cioè i punti più vicini all'iperpiano separatore (quelli doppiamente cerchiati in figura \ref{fig:lsd} sugli iperpiani $H_{1}$ e $H_{2}$ ) , hanno valori $\alpha_{i}$ non nulli. Pertanto solo tali punti contribuiranno al calcolo dei pesi $w$ e per tale motivo sono chiamati \textbf{vettori di supporto}.\\
La formulazione duale del problema è detta \textbf{Wolf duale}, vediamo come si ottiene. Si sostituiscono i vincoli di eguaglianza \eqref{wvar} e \eqref{bvar} riguardanti $b$ e $w$ ($\frac{\partial L_{p}}{\partial w} = 0 \text{ e } \frac{\partial L_{p}}{\partial b} = 0$) nella formulazione primaria cioè nell'equazione \eqref{eq:pri}. Si ha:
\begin{equation}
\begin{split}
&L_{p} = \frac{1}{2}\norma{w}^{2} - \sum_{i=1}^{l}\alpha_{i}y_{i}(w \bullet x_{i} + b) + \sum_{i=1}^{l}\alpha_{i} = \frac{1}{2}(w \bullet w) - \sum_{i=1}^{l}\alpha_{i}y_{i}(w \bullet x_{i} ) \quad\:\: -\\
 &- \sum_{i=1}^{l}\alpha_{i}y_{i}b + \sum_{i=1}^{l}\alpha_{i}
\underset{Si \:sostituisce \:l'equazione \:\eqref{wvar}}{=} \frac{1}{2} \Biggl(\sum_{i=1}^{l}\alpha_{i}y_{i}x_{i} \bullet \sum_{i=1}^{l}\alpha_{i}y_{i}x_{i} \Biggl) \quad\:\:\:\, - \\&
- \Biggl(\sum_{i=1}^{l}\alpha_{i}y_{i}\Bigl(x_{i} \bullet \sum_{j=1}^{l}\alpha_{j}y_{j}x_{j}\Bigl)\Biggl) -\:b\sum_{i=1}^{l}\alpha_{i}y_{i} + \sum_{i=1}^{l}\alpha_{i}
\underset{b \:sparisce\:per \:l'equazione\:\eqref{bvar}}{=} \frac{1}{2} \\&\sum_{i,j = 1}^{l}\alpha_{i}\alpha_{j}y_{i}y_{j}\,(x_{i} \bullet x_{j}) - 
 \sum_{i,j = 1}^{l}\alpha_{i}\alpha_{j}y_{i}y_{j}\,(x_{i} \bullet x_{j}) + \sum_{i=1}^{l}\alpha_{i} \qquad \qquad \qquad \qquad \quad\:\:\,=\\
 &= - \frac{1}{2} \sum_{i,j = 1}^{l}\alpha_{i}\alpha_{j}y_{i}y_{j}\,(x_{i} \bullet x_{j}) + \sum_{i=1}^{l}\alpha_{i}
\end{split}
\end{equation}
quindi si è trovato che la formulazione duale da massimizzare è:
\begin{alignat}{4}
& L_{d} = - \frac{1}{2} \sum_{i,j = 1}^{l}\alpha_{i}\alpha_{j}y_{i}y_{j}\,(x_{i} \bullet x_{j}) + \sum_{i=1}^{l}\alpha_{i} \label{eq:dual}\\
& soggetta \:ai\:vincoli: \nonumber\\
&\sum_{i=1}^{l}\alpha_{i}y_{i} = 0 \quad \forall i \\
&\alpha_{i} \geq 0 \qquad \quad\:\:\: \forall i
\end{alignat}
Si procede come per la forma primale,  cioè imponendo che il lagrangiano della forma duale $L_{d}$ sia pari a zero. Si risolve il sistema, in cui la dipendenza da $w$ e $b$ è sparita (visionare equazione \eqref{eq:dual}), e si trovano gli $\alpha_{i}$.\\
 L'obiettivo finale è trovare l'iperpiano $f(x) = w \bullet x +b$ con i $w \text{ e } b$ cioè la coppia $(w,b)$, che massimizzano il margine. Mediante la formulazione duale si trovano gli $\alpha_i$ che garantiscono i ``migliori'' $(w,b)$ e vengono sosti,tuiti nell'equazione \eqref{wvar} per calcolare il vettore $w$ ricordando che soltanto i \textit{vettori di supporto} ,che hanno $\alpha_{i} \neq 0$, contribuiscono al calcolo di $w$. Quindi si ha: 
\begin{alignat}{3}
f(x)&= w \bullet x +b = \Biggl(\sum_{i=1}^{l}\alpha_{i}y_{i}x_i\Biggl) \bullet x +b = \nonumber\\
&=\Biggl(\sum_{i=1}^{l}\alpha_{i}y_{i}(x_i \bullet x)\Biggl) +b =\nonumber\\
&= \Biggl(\sum_{i \in sv}^{}\alpha_{i}y_{i}(x_i \bullet x)\Biggl) +b \label{eq:test}
\end{alignat} 
Per calcolare $b$ si sostituisce in \ref{eq:test} un elemento $x$ del \textit{training set} --- ed anche la corrispondente etichetta $y$ --- che rappresenta un vettore di supporto, ed i valori ottenuti vengono poi mediati quindi:
\begin{equation}
\label{eq:bhard}
b = \frac{1}{\abs{sv}} \sum_{i \in sv}^{}\Biggl(y_i - \Bigl(\sum_{j \in sv}^{}\alpha_{j}y_{j}(x_i \bullet x_j)\Bigl)\Biggl)
\end{equation}

\subsubsection{Fase di Test}
Una volta trovati $w$ e $b$, e quindi una volta che la \ac{SVM} è stata \textit{trained} come si usa il classificatore trovato? Immaginiamo di avere un campione n-dimensionale del \textit{test set} $m$ allora si usa l'iperpiano $w \bullet x +b$ per classificare il campione \emph{unseen} $m$ in base all'esito di $sign(w \bullet f + b)$ (l'etichetta di m è 1 oppure -1 in base a $sign(w \bullet m + b)$)

\subsection{Soft Margin}
L'esposizione della sottosezione \ref{sub:cmm} è valida per un \textit{training set} linearmente separabile. Nella maggiore parte dei casi reali i dati tipicamente non sono linearmente separabili cioè non ci sarà una \textit{``feasible region''} perchè non esisterà nessun valore $(w,b)$ ,cioè nessun iperpiano , per cui i vincoli \ref{eqn:h1} e \ref{eqn:h2} siano rispettati. L'idea ,in \cite{Cortes95},è di rilassare tali vincoli  mediante l'introduzione delle \textit{\textbf{variabili slack}} così definite:
\begin{definizione*}
Detta $f$ la funzione caratteristica un classificatore, che ha come dominio il training set $X$, e un campione $\in X (x_i , y_i)$, si definisce variabile slack del campione, rispetto alla funzione $f$ e al margine $\gamma$, la quantità :
\begin{equation*}
\xi((x_i , y_i) , f , \gamma  ) = \xi_{i} = max(0 , \gamma - y_if(x_i))
\end{equation*}
\end{definizione*}
e il \textit{vettore slack} di $X$ come:
\[ 
\xi = \Braket{\xi_1,\xi_2,\dots,\xi_l}
\] 
Si noti che $\xi_i > \gamma$ indica un'errata classificazione del campione $x_i$.
Quindi i vincoli \ref{eqn:h1} e \ref{eqn:h2} sul \textit{training set} vengono riformulati definendo:
\begin{align}
&w \bullet x_i + b \geq \:\:\:1-\xi_i \qquad \qquad\text{ per } y_i = +1 \label{eqn:h1sm}\\
&w \bullet x_i + b \leq -1+\xi_i \qquad \qquad\text{ per } y_i = -1  \label{eqn:h2sm}\\
&\xi_i \geq 0 \quad \qquad \qquad \qquad \qquad \quad\:\: \forall i \label{eqn:slack}
\end{align}
Un errore in \eqref{eqn:h1sm} e \eqref{eqn:h2sm} occorre quando il corrispondente $\xi_i$ eccede l'unità, ne consegue che $\sum_{i=1}^{l}\xi_i$ è un limite superiore al numero di errori sul \textit{training set}. Allora la funzione obiettivo da minimizzare diventa:
\begin{equation}
\label{eq:softorig}
\frac{1}{2}\norma{w}^2 + Cf\Bigl(\sum_{i=1}^{l}\xi_i^k\Bigl)
\end{equation}
soggetta ai vincoli \eqref{eqn:h1sm} \eqref{eqn:h2sm} e \eqref{eqn:slack}. Il problema appena formulato è convesso  per qualunque $k$ ma si sceglie sempre $k=1$ perchè assicura l'unicità della soluzione (con $k=1$ ne le $\xi_i$ ne i moltiplicatori di Lagrange del problema primale appaiono nel Wolf duale problema come sarà accennato). La soluzione di questo problema di ottimizzazione è chiamato iperpiano \textbf{soft margin}. Inoltre la scelta maggiormente adottata è di approssimare\footnote{Si definisce la norma-1 di un vettore x a l componenti come $\norma{x}_1 = \sum_{i=1}^{l}\abs{x_i}$} $f\Bigl(\sum_{i=1}^{l}\xi_i\Bigl) \text{ con } \Bigl(\sum_{i=1}^{l}\xi_i\Bigl) = \norma{\xi}_1$ per questo talvolta si parla d'iperpiano soft margin norma-1.\footnote{Esiste anche la formulazione norma-2 ma non ci interessa approfondirla}. Il problema originale delineato in \eqref{eq:softorig}  minimizza il numero di errori di classificazione sull'insieme di addestramento ,e quindi minimizza il rischio empirico, ma è un problema NP-completo. Invece con l'approssimazione fatta si minimizza la norma di $\xi$  che non equivale più a minimizzare il numero di errori sui campioni del \textit{training set} ma consiste nel minimizzare gli scostamenti dei punti misclassificati dal nostro iperpiano  mentre determina l'iperpiano con margine massimo  per i restanti punti di $X$. E queso non è più un problema NP-completo. Ricapitolando la formulazione del problema di minimizzazione soft margin\footnote{Si intende da ora in avanti sempre norma-1} è :


\begin{alignat}{3}
\label{eq:softfor}
&minimizzare \quad&&\frac{1}{2}(w \bullet w) + C\sum_{i=1}^{l}\xi_i \\
\label{eq:softvinc}
&soggetto \:\:a &&y_i(w \bullet x_i + b) \geq \:\:\:1-\xi_i \qquad \qquad \\
&\:&&\xi_i \geq 0 \label{eq:softvin2}
\end{alignat}
La scelta di definire la funzione obiettivo \eqref{eq:softfor} tramite la norma risulta ottima dal punto di vista dell'errore di generalizzazione perchè si può dimostrare che quest ultimo è limitato superiormente da una quantità direttamente proporzionale alla norma dello \textit{slack vector} $\xi$, per cui minimizzare la norma consente di minimizzare anche l'errore di generalizzazione.

\subsubsection{Formulazione matematica}
Si  procede alla stessa maniera di hard margin ,cioè per dati linearmente separabili, su dati non linearmenti separabili con la differenza che la funzione obiettivo e anche i vincoli sono cambiati ( equazioni \eqref{eq:softfor} \eqref{eq:softvinc} \eqref{eq:softvinc2} ) per tollerare degli errori di classificazione del \textit{training set} e al contempo cercare di massimizzare il margine. Il problema primale diventa:
\begin{equation}
L_p = \frac{1}{2}\norma{w}^2 + C\sum_{i=1}^{l}\xi_i - \sum_{i=1}^{l}\alpha_{i}\{y_i(x_i \bullet w + b) - 1 + \xi_i\} - \sum_{i=1}^{l}\mu_i\xi_i
\end{equation}
Imponendo $\nabla L_p = 0$ e le KKT condizioni si ha:

\begin{subequations}
\begin{numcases}{}
\frac{\partial L_p}{\partial w} = \ w - \sum_{i=1}^{l}\alpha_{i}y_{i}x_{i} =0 \label{wdualsoft}\\
\frac{\partial L_p}{\partial b} = \sum_{i=1}^{l}\alpha_{i}y_{i} = 0 \notag\\
\frac{\partial L_p}{\partial \alpha_i} = y_{i}(w \bullet x_i + b) - 1 + \xi_i = 0 \quad \forall i \notag\\
\frac{\partial L_p}{\partial \xi_i} = C - \alpha_i - \mu_i = 0 \quad \forall i \notag\\
\xi_i \geq 0 \quad \forall i \notag\\
\alpha_i \geq 0 \quad \forall i \notag\\
\mu_i \geq 0 \quad \forall i \notag\\
 \mu_i\xi_i = 0 \quad \forall i \notag\\
 \alpha_i\{y_{i}(w \bullet x_i + b) - 1 + \xi_i\} = 0 \quad \forall i  \notag
\end{numcases}
\end{subequations}

Con dei calcoli analoghi a quanto fatto per il caso linearmente separabile si perviene alla forma duale che consiste nel:
\begin{alignat}{3}
\label{eq:softford}
&massimizzare \quad&&L_{d} = - \frac{1}{2} \sum_{i,j = 1}^{l}\alpha_{i}\alpha_{j}y_{i}y_{j}\,(x_{i} \bullet x_{j}) + \sum_{i=1}^{l}\alpha_{i} \\
\label{eq:softvincd}
&soggetto \:\:a &&0 \leq \alpha_i \leq C \qquad \qquad \\
\label{eq:softvind2}
&\:&&\sum_{i=1}^{l} \alpha_i y_i = 0 
\end{alignat}
Come per hard margin imponendo $\nabla L_d = 0$ e risolvendo si trovano gli $\alpha_i$ che sono usati in \ref{wdualsoft} per calcolare il vettore dei pesi $w$, cui come prima contribuiranno solo i vettori di supporto in quanto unico con $\alpha_i \neq 0$:
\begin{equation}
\label{eq:softw}
w = \sum_{i \in sv}^{}\alpha_iy_ix_i
\end{equation}
Il valore di b può essere ricavato come in \eqref{eq:bhard}.
Si osservi come la formulazione duale soft margin sia uguale , fatta eccezione per il vincolo sugli $\alpha_i$ , alla formulazione duale del problema del margine massimo. 

\subsubsection{Parametro C e legami con SRM}
Nella funzione obiettivo della formulazione soft margin \eqref{eq:softfor} compare il parametro $C$, talvota detto parametro \textbf{trade-off} , che è essenzialmente un parametro di \textbf{regolarizzazione} che regola il compromesso tra raggiungere un basso errore sul \textit{training set} e minimizzare $\norma{w}$ (cioè massimizzare il margine, che è $2/\norma{w}$ ed avere presumibilmente un minore errore di generalizzazione). Infatti nella forma duale l'unica dipendenza da C è negli $\alpha_i$ nell'equazione \eqref{eq:softvincd} ed avere un valore di $C$ piccolo limita il valore degli $\alpha_i$ e a sua volta del vettore $w$ che come si vede dall'equazione \eqref{eq:softw} aumenta all'aumentare degli $\alpha_i$. Quindi quando $C$ è più piccolo $w$ è pure più piccolo , e quindi il margine è più grande e di conseguenza per il teorema \ref{teo:suphdim} si avranno migliori performances di generalizzazione tuttavia un $C$ piccolo  comporta una minore \textit{accuracy} sul \textit{training set}. Viceversa , per gli stessi motivi, un valore di $C$ più grande classifica meglio il \textit{training set} ma avrà presumibilmente un maggiore errore di generalizzazione. Quindi la regolazione del parametro $C$ è uno step vitale perchè \ac{SRM} è parzialmente implementato tramite la regolazione di questo parametro dato che l'incremento di $C$ incrementa la complessità della classe delle ipotesi ,perchè in ultima analisi aumentare $C$ comporta l'aumento della \ac{VC} \textit{dimension} $h$. Molto spesso per trovare il migliore valore per $C$ si usa la validazione incrociata su vari valori del parametro. Quindi l'implementazione di \ac{SRM} nel soft margin è approssimata tramite la regolazione del parametro $C$.

\subsection{Kernels}
L'approccio soft margin per addestrare dati non linearmente separabili non garantisce usualmente ottime prestazioni. Un'alternativa molto potente è quella introdotta in \cite{Vapnik92}. L'idea per gestire dati non linearmente separabili è quella di mappare questi dati in uno spazio di maggiore dimensionalità $\mathcal{H}$ detto \textbf{spazio delle features} o \textbf{spazio delle caratteristiche} in cui i dati diventano lineramente separabili e poi applicare la tecnica hard margin. Questo mapping può essere fatto tramite una funzione $\phi$ siffatta:
\begin{equation*}
\phi : \mathbb{R}^n \to \mathcal{H}
\end{equation*}
L'immagine \ref{fig:kls} illustra quanto appena detto. 

\begin{figure}[htp]
	\centering
	\includegraphics[ width=0.5\textwidth]{KernelLinSepara}
	\caption[Mapping in feature space]{\textit{Dati in $\mathbb{R}^{2}$ resi linearmente separabili ,dalla funzione $\phi$, nello spazio di Hilbert di dimensione tre}}
   \label{fig:kls}
\end{figure}

Inoltre $(x_i \bullet x_j)$ nella formulazione duale in \eqref{eq:dual} diventa $(\phi(x_i) \bullet \phi(x_j))$. Tuttavia utilizzando il \textbf{kernel trick} è possibile pensare di usare una una \textbf{kernel function k} tale che
\begin{equation}
\label{eq:kerdef}
k(x_i , x_j) = \phi(x_i) \bullet \phi(x_j)
\end{equation}
che consente di usare il \textit{kernel} K nelle \ac{SVM} senza esplicitamente conoscere $\phi$.
Una puntualizzazione su $\phi$ è doverosa. Se $\mathcal{H}$ è di dimensione $M$ $\phi$ può essere una funzione che per calcolare ciascuna delle $M$ coordinate del generico campione $x \in \mathbb{R}^{d}$ nello spazio $H$ si usano $M$ differenti funzioni cioè:
\[
\phi(x) =[\phi_1(x),\dots,\phi_M(x)]
\]
e allora il kernel diventa:
\begin{equation}
k(x_i , x_j) = \phi(x_i) \bullet \phi(x_j) = \sum_{d=0}^{M}\phi_d(x_i)\phi_d(x_j)
\end{equation}
 Un esempio è il kernel in [INSERIRE RIFERIMENTO KERNEL GAUSSIANO] in cui $\mathcal{H}$ è di dimensione infinita. In quest ultimo esempio si può dimostare che $\mathcal{H}$ è di dimensione infinita così potrebbe non essere semplice lavorare con $\phi$ esplicitamente. Usando il \textit{kernel trick} si aggira il problema, infatti non va più svolto un prodotto scalare e due valutazioni della funzione $\phi()$ per ogni coppia $(x_i , x_j)$  ma la più semplice ed efficiente valutazione del kernel che sotto opportune condizioni del kernel assicura che implicitamente si sta svolgendo un prodotto scalare(ma senza svolgerne il calcolo esplicito che con alte o infinite dimensionalità di $\mathcal{H}$ come per il kernel gaussiano in[INSERIRE RIFERIMENTO KERNEL GAUSSIANO] è impossibile). Un altro esempio chiarificatore in cui i campioni $x$ del \textit{training set} appartengono allo spazio $\mathbb{R}^2$ è il seguente. Dato il \textit{kernel}:
 \begin{equation*}
 k(x_i,x_j) = (x_i \bullet x_j)^2
 \end{equation*}
 è facile trovare una funzione $\phi : \mathbb{R}^2 \to \mathcal{H}$ tale che si ha:
\begin{equation*}
 K(x,y) = (x \bullet y)^2 = \phi(x) \bullet \phi(y) \qquad \text{ con } x,y \in \mathbb{R}^2
\end{equation*}
Infatti se $x=\braket{x_1 , x_2} \text{ e } y=\braket{y_1 , y_2}$ si ha che
\begin{equation}
\label{eq:kernelex}
(x \bullet y)^2 = (x_1y_1 + x_2y_2)^2 = x_{1}^{2}y_{1}^{2} + x_{2}^{2}y_{2}^{2} + 2x_{1}y_{1}x_{2}y_{2}
\end{equation}
e scegliendo $\mathcal{H} = \mathbb{R}^3$ si può avere
 \begin{align*}
    \phi(x) &= \begin{pmatrix}
           x_{1}^2 \\
           \sqrt{2}\,x_{1}x_{2} \\
           x_{2}^2
         \end{pmatrix}
  \end{align*}
 
  quindi:
 \begin{align*}
    \phi(x) \bullet \phi(y) &= \begin{pmatrix}
           x_{1}^2 \\
           \sqrt{2}\,x_{1}x_{2} \\
           x_{2}^2
         \end{pmatrix}\bullet
         \begin{pmatrix}
           y_{1}^2 \\
           \sqrt{2}\,y_{1}y_{2} \\
           y_{2}^2
         \end{pmatrix} = x_{1}^{2}y_{1}^{2} + x_{2}^{2}y_{2}^{2} + 2x_{1}y_{1}x_{2}y_{2}
  \end{align*}

che coincide con quanto ottenuto in \eqref{eq:kernelex}. Inoltre fissato un \textit{kernel} non sono univoci ne i mapping $\phi$ nela dimensionalità dello spazio $\mathcal{H}$. Per esempio per il \textit{kernel} precedente si sarebbe potuto scegliere $\mathcal{H} = \mathbb{R}^3$ e 
 \begin{align*}
    \phi(x) &= \frac{1}{\sqrt{2}}\begin{pmatrix}
           (x_{1}^2 - x_{2}^2)\\
           2x_{1}x_{2} \\
           (x_{1}^2 + x_{2}^2)
         \end{pmatrix}
  \end{align*}
  oppure $\mathcal{H} = \mathbb{R}^4$ e
  \begin{align*}
    \phi(x) &= \begin{pmatrix}
           x_{1}^2\\
           x_{1}x_{2} \\
           x_{1}x_{2} \\
           x_{2}^2
         \end{pmatrix}
  \end{align*}
Per quanto riguarda $\mathcal{H}$ è uno spazio di Hilbert che generalizza lo spazio euclidiano classico, e ha intrinsecamente un \textit{inner product} definito. Comunque non ci si soffermerà su di esso piuttosto è importante comprendere in quali circostanze dato un \textit{kernel} esiste una coppia $(\mathcal{H} , \phi)$ con la proprietà \eqref{eq:kerdef}.
\begin{teorema}[Teorema di Mercer]Dato un kernel continuo e simmetrico\footnote{Simmetrico significa che k(x,y) = k(y,x).} nell'intervallo chiuso    $\overrightarrow{a} \leq \overrightarrow{x} \leq \overrightarrow{b}$ e $\overrightarrow{a} \leq \overrightarrow{y} \leq \overrightarrow{b}$. Allora k(x,y) può essere espanso come
\begin{equation}
k(\overrightarrow{x},\overrightarrow{y}) = \sum_{i=1}^{\infty}\lambda_i\,\phi_i(\overrightarrow{x})\phi_i(\overrightarrow{y})
\end{equation}
con $\lambda_i > 0$. Condizione necessaria e sufficiente affinchè tale espansione sia valida e la sua convergenza assoluta e uniforme è:
\begin{equation*}
\int_{\overrightarrow{b}}^{\overrightarrow{a}}\int_{\overrightarrow{b}}^{\overrightarrow{a}} g(\overrightarrow{x})k(\overrightarrow{x},\overrightarrow{y})g(\overrightarrow{y})\,d\overrightarrow{x}\,d\overrightarrow{y} \geq 0
\end{equation*}

\begin{equation*}
\text{per } \forall g(\cdot) \text{ : } \:\:\int_{\overrightarrow{b}}^{\overrightarrow{a}}g^2(\overrightarrow{x})\,d\overrightarrow{x} < \infty
\end{equation*}
\end{teorema}

Quindi una funzione \textit{kernel} che soddisfa le condizioni del teorema di Mercer rappresenta un prodotto scalare in uno spazio delle features ($\mathcal{H}$) generato da una qualche trasformazione non lineare. Si noti che tale spazio delle features $\mathcal{H}$ può essere infinito e il fatto che $\forall i \:\lambda_i > 0$ implica che il kernel K è definito positivo. Si ha che un kernel simmetrico K semidefinito positivo soddisfa le condizioni del Teorema di Mercer quindi condizione necessaria e sufficiente affinchè un Kernel rappresenti un prodotto scalare in qualche spazio di Hilbert $\mathcal{H}$ è che il kernel sia simmetrico semidefinito positivo.\footnote{\label{note:hessiano}Ad essere simmetrica definita positiva è la matrice associata al kernel. I valori della matrice sono calcolati sulle possibili coppie dei vettori del \textit{training set} cioè su $K(x_i,x_j) \text{ con } i,j=1,\dots,l$ e quindi possono essere raccolti in una matrice $K \in \mathbb{R}^l \times \mathbb{R}^l$ denominata matrice del kernel. E' questa matrice che deve essere simmetrica semidefinita positiva affinchè siano rispettate le condizioni del teorema di Mercer} 

\begin{table}[htp]
\centering
\[ 
\begin{array}{lc} 
\toprule
\text{\textbf{Tipo di Kernel}} & \text{\textbf{k(x , y)}}  \\
\midrule  
\text{Polinomiale}  &  (x \bullet y +1)^p \\
\text{Gaussiano}  &  e^{-\frac{\norma{x-y}^{2}}{2\sigma^2}}\\[1ex]
\text{Iperbolico}  & \tanh(\text{{\footnotesize {\textsl k}}}x \bullet y - \delta) \\

\bottomrule
\end{array}
\]
 \caption[Kernels più noti]{Tipi di kernels più conosciuti}
\label{tab:kerrem}
\end{table}  

Nella tabella \ref{tab:kerrem} si annoverano i kernel predefiniti più noti e maggiormente adottati. Un kernel di per sè non ci dice niente su $\phi$ e la dimensione di $\mathcal{H}$(spazio delle caratteristiche). Tuttavia è stato dimostrato che il \textit{kernel} polinomiale ha per  $\mathcal{H}$ una dimensione di $\binom{n+p-1}{p}$ dove si ricorda che $n$ è la dimensionalità di ogni campione del \textit{training set}. Invece il \textit{kernel} gaussiano ha una dimensionalità di $\mathcal{H}$ infinita. Il suo funzionamento è molto simile alle classiche \textit{Radial Basis Function} infatti  il numero di centri ($\abs{sv}$), i centri stessi (cioè i vettori di supporto) ed i pesi (gli$ \alpha_i$) sono prodotti automaticamente da \ac{SVM}. La quantità $\norma{x-y}$ rappresenta la distanza euclidea quadrata e quindi una misura di similarità e il parametro $\gamma = 1/2\sigma^2$ decide il peso di $y$ (se questo è un vettore di supporto) nell'influenzare la classificazione di $x$. Un $\gamma$ piccolo significa un gaussiano con una grande varianza e quindi $y$ (si sottintende che $y$ è un vettore di supporto) avrà un'influenza rilevante su $x$ anche se la distanza tra $x$ e $y$ è grande, al contrario se $\gamma$ è grande la varianza del gaussiano è piccola e $y$ non avrà una influenza molto significativa su $x$ (specie se molto distanti). Le prestazioni del \textit{gaussiano} danno risultati eccellenti paragonate alle classiche RBF.  Il \textit{kernel} iperbolico è detto anche neurale perchè ricalca da vicino una rete neurale ed in particolare un'unità sigmoidale a due strati. Il primo strato consiste in $\abs{sv}$ insiemi di pesi, ed ogni insieme consta $n_l$ (la dimensionalità dei campioni) pesi, e il secondo strato consiste di $\abs{sv}$ pesi (gli $\alpha_i$) cosiché una valutazione richiede di prendere una somma pesata dei sigmoidi, essi stessi valutati sul prodotto scalare dei dati nel \textit{test set} con i vettori di supporto. Il \textit{kernel} iperbolico-neurale è poco utilizzato perchè solo per alcuni specifici valori di {\footnotesize {\textsl k}} ,$\delta$ e dei dati $x$ le condizioni del teorema di Mercer sono soddisfatte ed è stato dimostrato che molte combinazioni dei parametri ci si riconduce al \textit{kernel} gaussiano.\\
La ragione per mappare i campioni dallo spazio originario in uno spazio a dimensione molto superiore (\textit{feature space} che come visto per il \textit{kernel} gaussiano può essere anche infinita è il \textbf{teorema di Cover sulla separabilità} \cite{Cover65}:
\begin{teorema}
\label{teo:Cover}
Un problema di classificazione complesso, formulato attraverso una classificazione non lineare dei dati ad altà dimensionalità, ha maggiore probabilità di essere linearmente separabile che in uno spazio a bassa dimensionalità.
\end{teorema}

\subsubsection{Formulazione matematica}
Come indicato dal teorema \ref{teo:Cover} nello spazio $\mathcal{H}$ si avrà presumibilmente una lineare separazione dei dati e questo consente di utilizzare la formulazione hard margin in \eqref{eq:dual}  che resta pressochè invariata. L'unica sostituzione da fare è nell'utilizzo del mapping nel prodotto scalare. Quindi la nuova formulazione  con kernel diventa:

\begin{alignat}{4}
& L_{d} = - \frac{1}{2} \sum_{i,j = 1}^{l}\alpha_{i}\alpha_{j}y_{i}y_{j}\,(\phi(x_{i}) \bullet \phi(x_{j})) + \sum_{i=1}^{l}\alpha_{i} = - \frac{1}{2} \sum_{i,j = 1}^{l}\alpha_{i}\alpha_{j}y_{i}y_{j}\,k(x_{i} ,  x_{j}) + \sum_{i=1}^{l}\alpha_{i} \label{eq:Kerdual}\\
& soggetta \:ai\:vincoli: \nonumber\\
&\sum_{i=1}^{l}\alpha_{i}y_{i} = 0 \quad \forall i \nonumber\\
&\alpha_{i} \geq 0 \qquad \quad\:\:\: \forall i \nonumber
\end{alignat}

Anche in fase di test non ci sono problemi dato che  la classificazione del campione di test $x$ scaturisce dalla valutazione di $sign(w \bullet \phi(x) +b)$ ma $w = \sum_{i =1}^{\abs{sv}}y_i\alpha_i\phi(x_i)$ dove gli $x_i$ sono i vettori di supporto. Quindi sostituendo si ha:
\begin{equation}
sign\Bigl(\sum_{i=1}^{\abs{sv}}\alpha_iy_i\phi(x_i) \bullet \phi(x) + b \Bigl)= sign\Bigl(\sum_{i=1}^{\abs{sv}}\alpha_iy_i\,k(x_i , x) + b\Bigl)
\end{equation}
quindi anche in fase di test non abbiamo la necessità di calcolare $\phi$ che comporterebbe l'effettuazione di prodotti scalari di vettori con un numero di componenti eleveto se non addirittura infinito (come il \textit{kernel} gaussiano). E' proprio questo il vantaggio che apporta il \textbf{kernel trick}.

\subsubsection{Prodotto scalare come misura di similarità}
\label{subsub:prscal}
Il motivo per cui si predilige la formulazione duale rispetto a quella primaria è da ricercare nel fatto che in essa compare il prodotto scalare che come visto conduce alla tecnica dei \textit{kernels}. Un altro motivo è che il prodotto scalare è una \textbf{misura di similarità} e quindi trascendendo i \textit{kernels} classici della tabella \ref{tab:kerrem} ,se si rivelano inappropriati, è possibile definire un \textit{kernel} su misura che implicitamente definisce una misura di similarità \textit{customizzata} tra i campioni. Riferendoci ,senza perdere di generalità,alla formulazione duale hard margin in \eqref{eq:dual} \footnote{Per la formulazione con kernels la misurà di similarità è relativa ai campioni trasformati cioè a $\phi(x_i) \bullet \phi(x_j)$} si cerca di interpretare più in dettaglio come funziona il prodotto scalare ricordando che $L_d$ va massimizzato. Assumendo che $x_i$ e $x_j$ sono due campioni  si ha che:
\begin{enumerate}
\item Se $x_i \text{ e } x_j$ sono \textbf{completamente dissimili} come in figura \ref{fig:dissp} il prodotto scalare è zero e non contribuiscono a $L_d$ (anche se non sono esattamente otogonali ma quasi il prodotto scalare sarà piccolo ed anche il contributo a $L_d$).
\item Se $x_i \text{ e } x_j$ sono \textbf{completamente simili} (o quasi) cioè il loro prodotto scalare è uno (o quasi uno) si può fare un ulteriore distinguo:
\begin{itemize}
\item $y_i \text{ e } y_j$ cioè le rispettive etichette sono uguali (cioè entrambe 1 o entrambe -1), allora siccome $\alpha_i,\alpha_j \geq 0$ $\alpha_i\alpha_jy_iy_j(x_i \bullet x_j)$ per quei due specifici campioni $x_i,x_j$ è positivo (o nullo) ma moltiplicato per -1/2 è negativo quindi $L_d$ diminuisce. Se ne deduce che campioni simili (prodotto scalare uguale o vicino a uno) con la stessa etichetta come in figura \ref{fig:simpsamelab} sono ignorati da \ac{SVM} (perchè altrimenti il margine diminuirebbe) e quindi \ac{SVM} non li rende vettori di supporto.
\item $y_i \text{ e } y_j$ cioè le rispettive etichette sono opposte (cioè 1 e -1 o viceversa). Inoltre i vettori sono simili e il prodotto scalare è vicino a 1 come in figura \ref{fig:simpdifflab} quindi $\alpha_i\alpha_jy_iy_j(x_i \bullet x_j) \leq 0$ e moltiplicato per -1/2 diventa positivo e quindi contribuisce ad incrementare il margine. Quindi \ac{SVM} renderà tali elementi del \textit{training set} vettori di supporto (perchè fanno aumentare il margine) 
\end{itemize}
\end{enumerate}

\begin{figure}[htp]
	\centering
	\includegraphics[ width=0.5\textwidth]{VettoriDissimili}
	\caption[Campioni dissimili]{\textit{Due campioni dissimili(ortogonali), indipendentemente dall'etichetta, non contano affatto.}}
   \label{fig:dissp}
\end{figure}

\begin{figure}[htp]
	\centering
	\includegraphics[ width=0.5\textwidth]{VettoriSimiliDifferenteLabel}
	\caption[Campioni simili differenti etichette]{\textit{Due campioni molto simili,$x_i$ e $x_j$, con etichette diverse, tendono a massimizzare il margine e sono resi vettori di supporto.}}
   \label{fig:simpdifflab}
\end{figure}

\begin{figure}[htp]
	\centering
	\includegraphics[ width=0.5\textwidth]{VettoriSimiliUgualeLabel}
	\caption[Campioni simili differenti etichette]{\textit{Due campioni molto simili,$x_i$ e $x_j$, con etichette uguali, tendono a minimizzare il margine e quindi non vanno resi vettori di supporto. Oppure se uno di loro è un vettore di supporto (perchè con altri campioni contribuisce ad aumentare il margine) l'altro non va reso un vettore di supporto (in modo da avere coefficiente nullo) in modo da non influenzare negativamente il margine.}}
   \label{fig:simpsamelab}
\end{figure}

\subsubsection{String kernels}
Come accennato in precedenza in \ref{subsub:prscal} determinati ambiti è possibili definire un \textit{kernel} che rappresenta una misura di similarità adatta allo specifico problema da affrontare o in cui gli elementi del \textit{training set} sono strutturati, come ad esempio stringhe ed alberi. Un esempio molto importante , e rilevante ai fini della tesi, è quello dei \textbf{string kernels}, dei quali  è possibile trovare un'introduzione in \cite[p. 60]{DeLaHiguera10}. In questa sede a titolo esemplificativo se ne descriveranno un paio che saranno menzionati più avanti. In generale l'obiettivo di uno \textit{string kernel} è quello di rappresentare o estrapolare delle features delle stringhe che possono essere rilevanti per un problema specifico in termini di similarità o dissimilarità. Uno dei più semplici è il \textbf{Parikh kernel} che associa ad ogni stringa un vettore di numeri naturali dove ogni componente è il numero di occorrenze di un simbolo dell'alfabeto nella stringa. La dimensione del \textit{feature space} dipende dal numero di elementi nell'alfabeto. Si consideri l' esempio nella tabella \ref{tab:parker} in cui si hanno quattro elementi del \textit{training set} $\{bac , baa , cab , bad\}$ e i quattro simboli dell'alfabeto $\{a,b,c,d\}$ che rappresentano le funzioni $\phi_{1}\text{,}\phi_{2}\text{,}\phi_{3}\text{,}\phi_{4}$ che mappano gli elementi nel \textit{feature space}. Si ha allora che la valutazione del \textit{kernel} tra gli elementi $bad \text{ e } baa$ è
\begin{equation*}
k(bad , baa) = 1\cdot2 + 1\cdot1 + 0\cdot0+1\cdot0 = 3
\end{equation*}
Un altro \textit{string kernel} molto noto è \textbf{all k-subsequences kernel} che calcola il prodotto scalare di due vettori in cui ogni componente è il numero di occorrenze di sottosequenze (che si ricorda possono essere contigue o non contigue) di lunghezza al più $k$ in una stringa. Nella tabella \ref{tab:all2subker} vi un esempio di \textit{All 2-subsequences kernel}.\footnote{Dalla tabella nell'esempio per motivi di spazio sono omesse le \textit{features}, cioè le sottosequenze , per le quali tutti i campioni hanno valore zero come ad esempio la sottosequenza $cc$} Si ha allora che la valutazione del \textit{kernel} nell'esempio tra gli elementi $bad \text{ e } baa$ è
\begin{equation*}
k(bad , baa) =  6
\end{equation*} 
\textbf{All k-subsequences kernel} ha sempre almeno valore 1 per la presenza della $feature$ $\lambda$ cioè la stringa vuota che è sottosequenza di ogni stringa. Anche per i \textit{string kernels} tipicamente non si calcolano le funzioni $\phi$ e irelativi prodotti scalari ma si usa il \textit{kernel trick} che viene realizzato tramite tecniche di programmazione dinamica. Per un'implementazione di molti dei più noti \textit{string kernels} compresi quelli esposti in questa sede si veda \cite{Cristianini04}.
I kernel non sono solo un escamotage per renderne efficiente il calcolo, che comunque in alcuni casi come uno spazio delle caratteristiche enorme o infinito sarebbe impossibile, ma hanno due pregi notevoli che rendono \ac{SVM} molto versatile ed applicabile nei più svariati contesti:
\begin{enumerate}
\item Consentono di trattare con elementi del \textit{training set} che non sono in forma vettoriale ma che sono strutturati come ad esempio alberi e grafi
\item Consentono di trattare con elementi del \textit{training set} che hanno lunghezza differente infatti finora si è sottaciuto che le \ac{SVM} nella loro formulazione senza \textit{kernel} possono funzionare solo con elementi della stessa lunghezza. Una trasformazione $\phi$, implicitamente indotta da un kernel che rispetta le condizioni di Mercer, uniforma le stringhe alla stessa lunghezza $M$ (M è la dimensione dello spazio di Hilbert $\mathcal{H}$) e quindi permette di trattare anche campioni di lunghezza diversa.
\end{enumerate}

\begin{table}[htp]
\centering 
\begin{tabular}{lcccc} 
\toprule
\multirow{2}*{Dati} &  $\phi_1$ &  $\phi_2$ &  $\phi_3$ &  $\phi_4$\\
\cmidrule(lr){2-5}
& $a$ & $b$ & $c$ & $d$  \\
 \midrule  
bac  &  1 & 1 & 1 & 0 \\ 
baa  &  2 & 1 & 0 & 0 \\
cab  &  1 & 1 & 1 & 0 \\
bad  &  1 & 1 & 0 & 0 \\
\bottomrule
\end{tabular}
\caption[Parikh kernel]{\textit{Parikh kernel su quattro stringhe e su un alfabeto quaternario}}
\label{tab:parker}
\end{table}    




\begin{table}[htp]
\centering 
\begin{tabular}{lcccccccccccccc} 
\toprule
\multirow{2}*{Dati} &  $\phi_1$ &  $\phi_2$ &  $\phi_3$ &  $\phi_4$ &  $\phi_5$ &  $\phi_6$ &  $\phi_7$ &  $\phi_8$ &  $\phi_9$ &  $\phi_{10}$ &  $\phi_{11}$ &  $\phi_{12}$ &  $\phi_{13}$ &  $\phi_{14}$\\
\cmidrule(lr){2-15}
& $\lambda$ & $a$ & $b$ & $c$ & $d$ & $aa$ & $ab$ & $ac$ & $ad$ & $ba$ & $bc$ & $bd$ & $ca$ & $cb$  \\
 \midrule  
bac  &  1 & 1 & 1 & 1 &  0 & 0 & 0 & 1  &  0 & 1 & 1 & 0 &  0 & 0 \\ 
baa  &  1 & 2 & 1 & 0 &  0 & 1 & 0 & 0  &  0 & 2 & 0 & 0 &  0 & 0 \\
cab  &  1 & 1 & 1 & 1 &  0 & 0 & 1 & 0  &  0 & 0 & 0 & 0 &  1 & 1 \\
bad  &  1 & 1 & 1 & 0 &  1 & 0 & 0 & 0  &  1 & 1 & 0 & 1 &  0 & 0 \\
\bottomrule
\end{tabular}
 \caption[All 2-subsequences kernel]{\textit{All 2-subsequences kernel su quattro stringhe e su un alfabeto quaternario}}
\label{tab:all2subker}
\end{table}

\subsection{Versione soft margin kernelized}
Le tecniche esposte nelle sottosezioni [INSERIRE RIFERIMENTI SOFT E KERNELS] sono combinate insieme per ottenere la versione delle \ac{SVM} di riferimento maggiormente adottata. Infatti sebbene i  \textit{kernels} rendano presumibilmente il \textit{training set} linearmente separabile non c'è alcuna garanzia che questo avvenga realmente. Inoltre anche se i deti vengono resi linearmente separabili è consigliabile utilizzare ugualmente la tecnica soft margin anziche quella del classificatore a margine massimo (hard margin) perchè può accadere che a causa della presenza di alcuni \textbf{\textit{outliers}}\footnote{Sono delle anomalie cioè degli elementi del \textit{training set}, tipicamente in numero esiguo, che si discostano nettamente dal resto dei campioni} ci sia \textit{overfitting} , cioè eccessivo adattamento al \textit{training set} e scarsa capacità di generalizzare , che potrebbe essere scongiurato scegliendo un \textbf{decision boundary}\footnote{cioè un iperpiano separatore} con soft margin che anche se ignora gli \textit{outliers} misclassificandoli porta alla scelta di un iperpiano separatore con un margine più grande e quindi con migliori capacità di generalizzare. Per questa ragione anche se la scelta di un kernel rendesse i dati lineramente separabili (asserzione che comunque non è sempre verificata) si predilige soft margin ad hard margin per cui usualmente la versione di riferimento di \ac{SVM} combina i \textit{kernels} con soft margin e la formulazione matematica del problema duale da risolvere diventa:
\begin{alignat}{3}
\label{eq:softford}
&massimizzare \quad&&L_{d} = - \frac{1}{2} \sum_{i,j = 1}^{l}\alpha_{i}\alpha_{j}y_{i}y_{j}\,k(x_{i} , x_{j}) + \sum_{i=1}^{l}\alpha_{i} \\
\label{eq:softvincd}
&soggetto \:\:a &&0 \leq \alpha_i \leq C \qquad \qquad \\
\label{eq:softvind2}
&\:&&\sum_{i=1}^{l} \alpha_i y_i = 0 
\end{alignat}

\subsection{Globalità e unicità della soluzione}
Uno dei vantaggi delle \ac{SVM} rispetto alle reti neurali è che assicurano una soluzione deterministica nel senso che dato un \textit{training set} la soluzione non dipende da elementi aleatori cosa che accade nelle reti neurali dove l'inizializzazione casuale dei pesi può condurre a una soluzione differente da un'esecuzione all'altra. Un altro vantaggio delle \ac{SVM} è che essendo un problema di programmazione quadratica convessa con vincoli lineari assicura di trovare una soluzione globale mentre nelle reti neurali si ha la certezza di trovare un minimo locale ma non è assicurato di trovare la soluzione migliore. Nelle \ac{SVM} il rispetto delle  KKT condizioni è una condizione necessaria per trovare un minimo o un massimo globale ma in generale non sono condizioni sufficienti cioè vi è solo la garanzia di un minimo locale. Tuttavia la natura convessa del problema assicura che la soluzione trovata sia sempre quella globale e questo è il motivo per cui si usa la formulazione quadratica alternativa in \ref{eq:svmpro1}. Ma è necessario garantire anche l'\textbf{unicità} della soluzione $\{w,b\}$ perchè potrebbero esistere un'altra coppia $\{w,b\}$ che sia pure soluzione (esistenza più minimi globali equivalenti) oppure la stessa soluzione, ma con una diversa espansione di $w=\sum_{i=1}^{\abs{sv}}\alpha_iy_ix_i$ e questo è pure rilevalente perchè potrebbe essere un'espansione migliore con meno vettori di supporto\footnote{Una soluzione con tanti vettori di supporto vicino al numero di elementi nel \textit{training set} è indice di \textit{overfitting}.}.
E' garantito che la soluzione sia unica se la formulazione duale \ref{eq:dual} o la formulazione duale con \textit{kernel} \ref{eq:Kerdual} è strettamente convessa. Anzichè le formulazioni \ref{eq:dual} e \ref{eq:Kerdual} si può definire l'\textbf{Hessiano} del problema che è una formulazione matriciale ottenuta come descritto nella nota \ref{note:hessiano}. Si ha che:
\begin{teorema*} Un problema quadratico è strettamente convesso se e solo se l'Hessiano associato è definito positivo.
\end{teorema*}
Quando l'Hessiano non è definito positivo la natura convessa del problema assicura che l'Hessiano sia comunque  almeno semidefinito positivo cosa che assicurà la globalità della soluzione ma non l'unicità.  Riassumendo la globalità della soluzione è sempre garantita. L'unicità è garantita solo se l'Hessiano è definito positivo. 